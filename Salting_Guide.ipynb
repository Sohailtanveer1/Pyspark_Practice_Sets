{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Taming Data Skew in Spark with Salting ‚Äî An End-to-End Guide**\n",
        "\n",
        "If you‚Äôve ever run a Spark job that was supposed to finish in minutes but ended up running for an eternity (or failed altogether), you might have met the sneaky culprit: **data skew**.\n",
        "\n",
        "Data skew happens when certain keys in your dataset have way more records than others. In a distributed system like Spark, this means that **some executors do way more work than others**, creating performance bottlenecks.\n",
        "\n",
        "In this post, we‚Äôll explore **salting** ‚Äî a simple but powerful trick to balance the load and speed up your Spark jobs. We‚Äôll walk through a **complete end-to-end example** so you can see exactly how it works."
      ],
      "metadata": {
        "id": "yfxIE5V93R9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is Data Skew?**\n",
        "\n",
        "Imagine you‚Äôre grouping transactions by customer_id to calculate the total amount spent..\n",
        "\n",
        "If **one customer** (say CUST001) has millions of transactions while others have just a few, Spark‚Äôs shuffle phase will send **all those millions of records to a single reducer.**\n",
        "\n",
        "This leads to:\n",
        "\n",
        "*   Slow stages\n",
        "*   Straggler tasks\n",
        "*   Out-of-memory errors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hx2rnKn7yipq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Idea Behind Salting**\n",
        "\n",
        "**Salting** means adding a **small, random or deterministic ‚Äúsalt‚Äù value** to your keys before expensive operations like **groupBy or join.**\n",
        "\n",
        "Instead of all records for CUST001 ending up in one partition, we spread them across multiple reducers. After the initial operation, we can **remove the salt** and combine results."
      ],
      "metadata": {
        "id": "WnpkUFSM30Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End Salting Example in PySpark**\n",
        "\n",
        "Let‚Äôs imagine we have:\n",
        "\n",
        "*  A large dataset of transactions with a customer_id\n",
        "*   We want to find total amount spent by each customer\n",
        "*   Problem: one customer_id (‚ÄúCUST001‚Äù) has millions of records ‚Üí skew.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OaUCh28R1KQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1 ‚Äî Sample Skewed Data**"
      ],
      "metadata": {
        "id": "CXQH5p-h1c5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGGwm4QAygTe",
        "outputId": "d1c369b6-7745-443b-cd9b-f515bfad5428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+\n",
            "|customer_id|amount|\n",
            "+-----------+------+\n",
            "|    CUST001|   224|\n",
            "|    CUST001|   343|\n",
            "|    CUST001|   179|\n",
            "|    CUST001|   254|\n",
            "|    CUST001|    53|\n",
            "+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit\n",
        "import random\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SaltingExample\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Skewed dataset\n",
        "data = []\n",
        "for i in range(1000000):\n",
        "    # Mostly customer_id = CUST001 (skewed)\n",
        "    cust_id = \"CUST001\" if random.random() < 0.7 else f\"CUST{random.randint(2, 50)}\"\n",
        "    amount = random.randint(10, 500)\n",
        "    data.append((cust_id, amount))\n",
        "\n",
        "df = spark.createDataFrame(data, [\"customer_id\", \"amount\"])\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "7k_VozbTzLuW",
        "outputId": "9117071b-284f-4141-c2ff-2f9e635ebe6d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x785c20c93ed0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://3bc52bae8daf:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>SaltingExample</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xXf7ob21nwl",
        "outputId": "30680339-9739-4ee7-9fe4-e65c1f614906"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"customer_id\") == \"CUST001\").count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGkcQ_7k4nqD",
        "outputId": "ace8c2d1-2eac-428d-c863-74814ad59445"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700638"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here, More than 70% of transactions belong to CUST001, simulating a skew.**"
      ],
      "metadata": {
        "id": "01Ftsxwu4mu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2 ‚Äî Without Salting (Skew Problem)**"
      ],
      "metadata": {
        "id": "b18EEK3o2CjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"customer_id\").sum(\"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJW64xaTzFI4",
        "outputId": "cf5b34d2-8802-43ef-d4be-fde2a4b1784a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|customer_id|sum(amount)|\n",
            "+-----------+-----------+\n",
            "|     CUST40|    1548054|\n",
            "|     CUST49|    1628669|\n",
            "|     CUST30|    1535821|\n",
            "|     CUST36|    1571761|\n",
            "|      CUST2|    1572690|\n",
            "|     CUST43|    1581272|\n",
            "|     CUST46|    1552256|\n",
            "|     CUST42|    1576348|\n",
            "|    CUST001|  178635654|\n",
            "|      CUST4|    1574003|\n",
            "|     CUST31|    1534098|\n",
            "|      CUST9|    1561539|\n",
            "|     CUST14|    1544118|\n",
            "|      CUST6|    1555607|\n",
            "|     CUST16|    1555347|\n",
            "|      CUST5|    1557201|\n",
            "|     CUST34|    1550529|\n",
            "|     CUST47|    1536868|\n",
            "|     CUST19|    1559625|\n",
            "|     CUST25|    1548677|\n",
            "+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we monitor the Spark UI Stage DAG, we‚Äôll notice:\n",
        "\n",
        "*   The key CUST001 goes mostly into 1 reducer ‚Üí unbalanced tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "sm5WB1ee1zz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3 ‚Äî Add Salt Key**"
      ],
      "metadata": {
        "id": "wPUD9eu51-pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add a small random number (salt) to distribute skewed keys."
      ],
      "metadata": {
        "id": "4cFTQoZi2RCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id, rand, floor, concat\n",
        "\n",
        "# Number of salts (adjust based on skew severity)\n",
        "num_salts = 5\n",
        "\n",
        "# Add a salt column (random int from 0 to num_salts-1)\n",
        "df_salted = df.withColumn(\"salt\", floor(rand() * num_salts))\n",
        "\n",
        "# Create a composite key: customer_id + salt\n",
        "df_salted = df_salted.withColumn(\"customer_salt\", concat(col(\"customer_id\") , lit(\"_\") , col(\"salt\").cast(\"string\")))\n",
        "\n",
        "df_salted.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDWO72XyzKVv",
        "outputId": "51b02adb-7a28-48d2-fe89-6ab604e83219"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+----+-------------+\n",
            "|customer_id|amount|salt|customer_salt|\n",
            "+-----------+------+----+-------------+\n",
            "|    CUST001|   224|   1|    CUST001_1|\n",
            "|    CUST001|   343|   0|    CUST001_0|\n",
            "|    CUST001|   179|   2|    CUST001_2|\n",
            "|    CUST001|   254|   2|    CUST001_2|\n",
            "|    CUST001|    53|   1|    CUST001_1|\n",
            "+-----------+------+----+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4 ‚Äî Aggregate on Salted Key**"
      ],
      "metadata": {
        "id": "MXhGvw6h2Kbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Aggregate with salted key\n",
        "agg_salted = df_salted.groupBy(\"customer_salt\").sum(\"amount\")\n",
        "\n",
        "agg_salted.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrXw_bGwzquJ",
        "outputId": "36b1c77e-8517-485c-8948-24d098f3943b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+\n",
            "|customer_salt|sum(amount)|\n",
            "+-------------+-----------+\n",
            "|     CUST13_4|     305319|\n",
            "|     CUST45_3|     321322|\n",
            "|     CUST50_1|     313721|\n",
            "|     CUST21_3|     318822|\n",
            "|      CUST8_2|     321668|\n",
            "+-------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Remove salt by grouping again\n",
        "from pyspark.sql.functions import split\n",
        "\n",
        "agg_final = agg_salted.withColumn(\"customer_id\", split(col(\"customer_salt\"), \"_\")[0]) \\\n",
        "                      .groupBy(\"customer_id\") \\\n",
        "                      .sum(\"sum(amount)\")\n",
        "\n",
        "agg_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzcbrHVK2eQN",
        "outputId": "90a7e7d5-4d59-43d8-927b-5c46435d96d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+\n",
            "|customer_id|sum(sum(amount))|\n",
            "+-----------+----------------+\n",
            "|     CUST40|         1548054|\n",
            "|     CUST49|         1628669|\n",
            "|     CUST36|         1571761|\n",
            "|     CUST30|         1535821|\n",
            "|      CUST2|         1572690|\n",
            "|     CUST43|         1581272|\n",
            "|     CUST46|         1552256|\n",
            "|     CUST42|         1576348|\n",
            "|    CUST001|       178635654|\n",
            "|      CUST4|         1574003|\n",
            "|     CUST31|         1534098|\n",
            "|      CUST9|         1561539|\n",
            "|     CUST14|         1544118|\n",
            "|      CUST6|         1555607|\n",
            "|     CUST16|         1555347|\n",
            "|      CUST5|         1557201|\n",
            "|     CUST34|         1550529|\n",
            "|     CUST47|         1536868|\n",
            "|     CUST19|         1559625|\n",
            "|     CUST25|         1548677|\n",
            "+-----------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **When to Use Salting**\n",
        "‚úÖ Use it when:\n",
        "\n",
        "\n",
        "*   You‚Äôve identified skewed keys in joins or aggregations.\n",
        "*   Spark UI shows uneven shuffle read/write sizes.\n",
        "\n",
        "\n",
        "\n",
        "üö´ Avoid it when:\n",
        "\n",
        "\n",
        "\n",
        "*   Your data is evenly distributed (it‚Äôll just add overhead).\n",
        "*   You have too many unique keys (salting might not be needed).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J1kMWFVP5Egg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Thoughts**"
      ],
      "metadata": {
        "id": "PCq0FLzl47qR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salting is a **low-cost, high-impact optimization** when dealing with skewed datasets in Spark. By distributing hot keys across multiple partitions, you prevent bottlenecks and keep your cluster running efficiently.\n",
        "\n",
        "If you regularly work with big data pipelines, **this is a trick worth keeping in your toolbox.**"
      ],
      "metadata": {
        "id": "0K-3MDlM45_G"
      }
    }
  ]
}