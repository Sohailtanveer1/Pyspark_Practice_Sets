{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1cac99d0",
      "metadata": {
        "id": "1cac99d0"
      },
      "source": [
        "# PySpark 50 Real-World Use Cases with Sample Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "It9aJkVNFqd-"
      },
      "id": "It9aJkVNFqd-"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3da8963c",
      "metadata": {
        "id": "3da8963c"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"50 Data Processing Use Cases\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set current date for time-based filters\n",
        "current_date = datetime.date(2023, 10, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e3378c",
      "metadata": {
        "id": "b1e3378c"
      },
      "source": [
        "## 1. Customer Insights - Top 5 Cities by Number of Customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f834fcce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f834fcce",
        "outputId": "7071b312-e6f5-464f-e162-9ec4ffba46d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|    city|count|\n",
            "+--------+-----+\n",
            "|  London|    2|\n",
            "|New York|    2|\n",
            "|  Berlin|    1|\n",
            "|   Paris|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "customers_data = [\n",
        "    (1, \"john@example.com\", \"New York\"),\n",
        "    (2, \"sarah@gmail.com\", \"London\"),\n",
        "    (3, \"mike@yahoo.com\", \"New York\"),\n",
        "    (4, \"lisa@example.com\", \"Paris\"),\n",
        "    (5, \"dave@outlook.com\", \"London\"),\n",
        "    (6, \"anna@gmail.com\", \"Berlin\")\n",
        "]\n",
        "customers = spark.createDataFrame(customers_data, [\"customer_id\", \"email\", \"city\"])\n",
        "\n",
        "# Solution\n",
        "top_cities = customers.groupBy(\"city\").count().orderBy(desc(\"count\")).limit(5)\n",
        "top_cities.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4637b12",
      "metadata": {
        "id": "a4637b12"
      },
      "source": [
        "## 2. Sales Analysis - Remove Rows with Null Invoice Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2b3e5c0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b3e5c0e",
        "outputId": "33b3e6b4-e692-4f67-eef8-28910c41cbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+------+\n",
            "|invoice_id|product_id|amount|\n",
            "+----------+----------+------+\n",
            "|      1001|      A101| 25.99|\n",
            "|      1003|      NULL| 15.49|\n",
            "|      1004|      C303|  75.0|\n",
            "+----------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "sales_data = [\n",
        "    (1001, \"A101\", 25.99),\n",
        "    (None, \"B202\", 49.99),\n",
        "    (1003, None, 15.49),\n",
        "    (1004, \"C303\", 75.00)\n",
        "]\n",
        "sales = spark.createDataFrame(sales_data, [\"invoice_id\", \"product_id\", \"amount\"])\n",
        "\n",
        "# Solution\n",
        "clean_sales = sales.dropna(subset=[\"invoice_id\"])\n",
        "clean_sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0086c0f5",
      "metadata": {
        "id": "0086c0f5"
      },
      "source": [
        "## 3. Marketing - Extract Email Domain from Email Addresses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8c0cebc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8c0cebc",
        "outputId": "c6e9194f-5ea6-4c54-f2c6-8d64f93b4fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----------+\n",
            "|           email|     domain|\n",
            "+----------------+-----------+\n",
            "|john@example.com|example.com|\n",
            "| sarah@gmail.com|  gmail.com|\n",
            "|  mike@yahoo.com|  yahoo.com|\n",
            "|lisa@example.com|example.com|\n",
            "|dave@outlook.com|outlook.com|\n",
            "|  anna@gmail.com|  gmail.com|\n",
            "+----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "email_domains = customers.withColumn(\"domain\", split(col(\"email\"), \"@\").getItem(1))\n",
        "email_domains.select(\"email\", \"domain\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1463c79",
      "metadata": {
        "id": "b1463c79"
      },
      "source": [
        "## 4. Retail: Products with Zero Quantity Sold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "92adb304",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92adb304",
        "outputId": "bc3a6970-c2b5-423f-ce45-1cba504044cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|product_name|quantity_sold|\n",
            "+----------+------------+-------------+\n",
            "|      P100|       Phone|            0|\n",
            "|      P300|      Tablet|            0|\n",
            "+----------+------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "inventory_data = [\n",
        "    (\"P100\", \"Phone\", 0),\n",
        "    (\"P200\", \"Laptop\", 12),\n",
        "    (\"P300\", \"Tablet\", 0),\n",
        "    (\"P400\", \"Monitor\", 5)\n",
        "]\n",
        "inventory = spark.createDataFrame(inventory_data, [\"product_id\", \"product_name\", \"quantity_sold\"])\n",
        "\n",
        "# Solution\n",
        "zero_sales = inventory.filter(col(\"quantity_sold\") == 0)\n",
        "zero_sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91598dba",
      "metadata": {
        "id": "91598dba"
      },
      "source": [
        "## 5. E-commerce: Customers with No Orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e720fe4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e720fe4d",
        "outputId": "b6bf76aa-d9d5-49d0-e94e-81e95e1b792c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+------+\n",
            "|customer_id|           email|  city|\n",
            "+-----------+----------------+------+\n",
            "|          2| sarah@gmail.com|London|\n",
            "|          6|  anna@gmail.com|Berlin|\n",
            "|          5|dave@outlook.com|London|\n",
            "|          4|lisa@example.com| Paris|\n",
            "+-----------+----------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "orders_data = [(101, 1, \"2023-10-01\"), (102, 3, \"2023-10-05\")]\n",
        "orders = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_date\"])\n",
        "\n",
        "# Solution\n",
        "no_orders = customers.join(orders, \"customer_id\", \"left_anti\")\n",
        "no_orders.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ba91a1",
      "metadata": {
        "id": "24ba91a1"
      },
      "source": [
        "## 6. Finance: High-Value Transactions (>$10K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "42d71435",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42d71435",
        "outputId": "59b78340-83a5-477e-8353-90729caf76ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-------+\n",
            "|txn_id|customer_id| amount|\n",
            "+------+-----------+-------+\n",
            "|  5002|          2|12000.0|\n",
            "|  5004|          4|15000.0|\n",
            "+------+-----------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "transactions_data = [\n",
        "    (5001, 1, 9500.0),\n",
        "    (5002, 2, 12000.0),\n",
        "    (5003, 3, 8500.0),\n",
        "    (5004, 4, 15000.0)\n",
        "]\n",
        "transactions = spark.createDataFrame(transactions_data, [\"txn_id\", \"customer_id\", \"amount\"])\n",
        "\n",
        "# Solution\n",
        "high_value_txns = transactions.filter(col(\"amount\") > 10000)\n",
        "high_value_txns.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79485a34",
      "metadata": {
        "id": "79485a34"
      },
      "source": [
        "## 7. Product Analysis: Top Products per Region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6676af71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6676af71",
        "outputId": "b785a82b-0b83-46f6-8360-8cae51db85a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+----+\n",
            "|region|product|sales|rank|\n",
            "+------+-------+-----+----+\n",
            "|  East| Laptop|  250|   1|\n",
            "| North| Laptop|  200|   1|\n",
            "| North|  Phone|  150|   2|\n",
            "| South|  Phone|  300|   1|\n",
            "| South| Tablet|  100|   2|\n",
            "|  West|  Phone|  180|   1|\n",
            "+------+-------+-----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "region_sales_data = [\n",
        "    (\"North\", \"Phone\", 150),\n",
        "    (\"North\", \"Laptop\", 200),\n",
        "    (\"South\", \"Phone\", 300),\n",
        "    (\"South\", \"Tablet\", 100),\n",
        "    (\"East\", \"Laptop\", 250),\n",
        "    (\"West\", \"Phone\", 180)\n",
        "]\n",
        "region_sales = spark.createDataFrame(region_sales_data, [\"region\", \"product\", \"sales\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"region\").orderBy(desc(\"sales\"))\n",
        "top_products = region_sales.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") <= 3)\n",
        "top_products.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac798d5",
      "metadata": {
        "id": "7ac798d5"
      },
      "source": [
        "## 8. Logistics: Total Weight by Country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4dc13505",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dc13505",
        "outputId": "90ed7dd8-cf67-4c28-fdc0-7081bbc00ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+\n",
            "|country|total_weight|\n",
            "+-------+------------+\n",
            "|    USA|         205|\n",
            "|     DE|         210|\n",
            "|     UK|          45|\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "shipments_data = [\n",
        "    (\"USA\", \"S1001\", 120),\n",
        "    (\"USA\", \"S1002\", 85),\n",
        "    (\"UK\", \"S2001\", 45),\n",
        "    (\"DE\", \"S3001\", 210)\n",
        "]\n",
        "shipments = spark.createDataFrame(shipments_data, [\"country\", \"shipment_id\", \"weight_kg\"])\n",
        "\n",
        "# Solution\n",
        "total_weight = shipments.groupBy(\"country\").agg(sum(\"weight_kg\").alias(\"total_weight\"))\n",
        "total_weight.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb131f3",
      "metadata": {
        "id": "9cb131f3"
      },
      "source": [
        "## 9. Customer Behavior: Age Group Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "955b09e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "955b09e9",
        "outputId": "e3d9f53e-3e9b-4aae-ee01-f0016f844bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+---------+\n",
            "|user_id|age|age_group|\n",
            "+-------+---+---------+\n",
            "|      1| 17|     Teen|\n",
            "|      2| 25|    Adult|\n",
            "|      3| 32|    Adult|\n",
            "|      4| 16|     Teen|\n",
            "|      5| 45|   Senior|\n",
            "|      6| 67|   Senior|\n",
            "|      7| 28|    Adult|\n",
            "|      8| 14|     Teen|\n",
            "+-------+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "users_data = [\n",
        "    (1, 17), (2, 25), (3, 32), (4, 16),\n",
        "    (5, 45), (6, 67), (7, 28), (8, 14)\n",
        "]\n",
        "users = spark.createDataFrame(users_data, [\"user_id\", \"age\"])\n",
        "\n",
        "# Solution\n",
        "age_groups = users.withColumn(\"age_group\",\n",
        "    when(col(\"age\") < 18, \"Teen\")\n",
        "    .when((col(\"age\") >= 18) & (col(\"age\") <= 35), \"Adult\")\n",
        "    .otherwise(\"Senior\")\n",
        ")\n",
        "age_groups.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d33aa0",
      "metadata": {
        "id": "e1d33aa0"
      },
      "source": [
        "## 10. Support Analysis: Tickets Resolved in 24h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "443c22fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443c22fb",
        "outputId": "a2b9d6bf-ea64-4516-aaae-7b8641ae22b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-------------------+----------------+\n",
            "|ticket_id|       created_time|      resolved_time|resolution_hours|\n",
            "+---------+-------------------+-------------------+----------------+\n",
            "|     T001|2023-10-01 09:00:00|2023-10-01 10:30:00|             1.5|\n",
            "|     T003|2023-10-03 11:00:00|2023-10-03 11:45:00|            0.75|\n",
            "+---------+-------------------+-------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "tickets_data = [\n",
        "    (\"T001\", \"2023-10-01 09:00:00\", \"2023-10-01 10:30:00\"),\n",
        "    (\"T002\", \"2023-10-02 14:00:00\", \"2023-10-03 15:00:00\"),\n",
        "    (\"T003\", \"2023-10-03 11:00:00\", \"2023-10-03 11:45:00\")\n",
        "]\n",
        "tickets = spark.createDataFrame(tickets_data, [\"ticket_id\", \"created_time\", \"resolved_time\"]) \\\n",
        "    .withColumn(\"created_time\", to_timestamp(col(\"created_time\"))) \\\n",
        "    .withColumn(\"resolved_time\", to_timestamp(col(\"resolved_time\")))\n",
        "\n",
        "# Solution\n",
        "resolved_24h = tickets.withColumn(\"resolution_hours\",\n",
        "        (col(\"resolved_time\").cast(\"long\") - col(\"created_time\").cast(\"long\")) / 3600) \\\n",
        "    .filter(col(\"resolution_hours\") <= 24)\n",
        "resolved_24h.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed784246",
      "metadata": {
        "id": "ed784246"
      },
      "source": [
        "## 11. Store Analytics: Average Basket Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cdff823e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdff823e",
        "outputId": "fecfd7d1-0926-48a2-a5e9-b3c8147a0120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+\n",
            "|store_id|avg_basket_size|\n",
            "+--------+---------------+\n",
            "|  StoreA|            6.5|\n",
            "|  StoreB|            7.5|\n",
            "+--------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "transactions_data = [\n",
        "    (\"StoreA\", \"T1001\", 5),\n",
        "    (\"StoreA\", \"T1002\", 8),\n",
        "    (\"StoreB\", \"T2001\", 3),\n",
        "    (\"StoreB\", \"T2002\", 12)\n",
        "]\n",
        "transactions = spark.createDataFrame(transactions_data, [\"store_id\", \"txn_id\", \"items\"])\n",
        "\n",
        "# Solution\n",
        "avg_basket = transactions.groupBy(\"store_id\").agg(avg(\"items\").alias(\"avg_basket_size\"))\n",
        "avg_basket.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a454138",
      "metadata": {
        "id": "7a454138"
      },
      "source": [
        "## 12. Healthcare: Patients with Missing Insurance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fa824119",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa824119",
        "outputId": "70843826-1994-475f-b740-a6e1557b83d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+---------+\n",
            "|patient_id|      name|insurance|\n",
            "+----------+----------+---------+\n",
            "|      P002|Jane Smith|     NULL|\n",
            "|      P004| Alice Kim|     NULL|\n",
            "+----------+----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "patients_data = [\n",
        "    (\"P001\", \"John Doe\", \"ABC Insurance\"),\n",
        "    (\"P002\", \"Jane Smith\", None),\n",
        "    (\"P003\", \"Bob Lee\", \"XYZ Insurance\"),\n",
        "    (\"P004\", \"Alice Kim\", None)\n",
        "]\n",
        "patients = spark.createDataFrame(patients_data, [\"patient_id\", \"name\", \"insurance\"])\n",
        "\n",
        "# Solution\n",
        "missing_insurance = patients.filter(col(\"insurance\").isNull())\n",
        "missing_insurance.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68fe832",
      "metadata": {
        "id": "c68fe832"
      },
      "source": [
        "## 13. Education: Average Score by Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cf4ab411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4ab411",
        "outputId": "c8545481-0335-4657-a8fd-9cb60398fc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|subject|overall_avg|\n",
            "+-------+-----------+\n",
            "|   Math|       81.5|\n",
            "|Science|       90.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "scores_data = [\n",
        "    (\"Math\", \"SchoolA\", 85),\n",
        "    (\"Math\", \"SchoolB\", 78),\n",
        "    (\"Science\", \"SchoolA\", 92),\n",
        "    (\"Science\", \"SchoolB\", 88)\n",
        "]\n",
        "scores = spark.createDataFrame(scores_data, [\"subject\", \"school\", \"avg_score\"])\n",
        "\n",
        "# Solution\n",
        "subject_avg = scores.groupBy(\"subject\").agg(avg(\"avg_score\").alias(\"overall_avg\"))\n",
        "subject_avg.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de808e99",
      "metadata": {
        "id": "de808e99"
      },
      "source": [
        "## 14. Employee Management: Small Departments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fdf03f0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf03f0a",
        "outputId": "c15d7150-9e7a-46d9-c329-99701c71feb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|dept|count|\n",
            "+----+-----+\n",
            "|  HR|    1|\n",
            "|  IT|    2|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "employees_data = [\n",
        "    (\"IT\", \"E001\"), (\"IT\", \"E002\"), (\"HR\", \"E003\"),\n",
        "    (\"Finance\", \"E004\"), (\"Finance\", \"E005\"), (\"Finance\", \"E006\")\n",
        "]\n",
        "employees = spark.createDataFrame(employees_data, [\"dept\", \"employee_id\"])\n",
        "\n",
        "# Solution\n",
        "small_depts = employees.groupBy(\"dept\").count().filter(col(\"count\") < 3)\n",
        "small_depts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9f8417",
      "metadata": {
        "id": "af9f8417"
      },
      "source": [
        "## 15. Subscription Services: Expiring Subscriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f6bf7ed6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bf7ed6",
        "outputId": "eeed0480-f076-45ba-84af-fc95466dc772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+--------------+\n",
            "|user_id|expiry_date|days_to_expiry|\n",
            "+-------+-----------+--------------+\n",
            "|   U001| 2023-10-20|          -611|\n",
            "|   U002| 2023-11-15|          -585|\n",
            "|   U003| 2023-10-30|          -601|\n",
            "|   U004| 2023-11-05|          -595|\n",
            "+-------+-----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from datetime import date\n",
        "\n",
        "# Sample Data\n",
        "subscriptions_data = [\n",
        "    (\"U001\", date(2023, 10, 20)),\n",
        "    (\"U002\", date(2023, 11, 15)),\n",
        "    (\"U003\", date(2023, 10, 30)),\n",
        "    (\"U004\", date(2023, 11, 5))\n",
        "]\n",
        "subscriptions = spark.createDataFrame(subscriptions_data, [\"user_id\", \"expiry_date\"])\n",
        "\n",
        "# Solution\n",
        "expiring_soon = subscriptions.withColumn(\"days_to_expiry\",\n",
        "        datediff(col(\"expiry_date\"), current_date())) \\\n",
        "    .filter(col(\"days_to_expiry\") <= 30)\n",
        "expiring_soon.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc02032f",
      "metadata": {
        "id": "dc02032f"
      },
      "source": [
        "## 16. Sales Forecasting: 7-Day Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "020fc423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "020fc423",
        "outputId": "d4138443-a455-4090-df3c-8b6b95dc2c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+------------------+\n",
            "|      date|sales|     7d_moving_avg|\n",
            "+----------+-----+------------------+\n",
            "|2023-10-01|  120|             120.0|\n",
            "|2023-10-02|  150|             135.0|\n",
            "|2023-10-03|  180|             150.0|\n",
            "|2023-10-04|   90|             135.0|\n",
            "|2023-10-05|  200|             148.0|\n",
            "|2023-10-06|  170|151.66666666666666|\n",
            "|2023-10-07|  210|             160.0|\n",
            "|2023-10-08|  190|             170.0|\n",
            "+----------+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "daily_sales_data = [\n",
        "    (\"2023-10-01\", 120), (\"2023-10-02\", 150), (\"2023-10-03\", 180),\n",
        "    (\"2023-10-04\", 90), (\"2023-10-05\", 200), (\"2023-10-06\", 170),\n",
        "    (\"2023-10-07\", 210), (\"2023-10-08\", 190)\n",
        "]\n",
        "daily_sales = spark.createDataFrame(daily_sales_data, [\"date\", \"sales\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
        "moving_avg = daily_sales.withColumn(\"7d_moving_avg\", avg(\"sales\").over(window_spec))\n",
        "moving_avg.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a634d3b",
      "metadata": {
        "id": "6a634d3b"
      },
      "source": [
        "## 17. Web Analytics: Top Pages per Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "037e9ef8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "037e9ef8",
        "outputId": "73e6cef6-5001-4d4c-bc6a-e4314ff3883c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+----+\n",
            "|session_id|     page|duration_sec|rank|\n",
            "+----------+---------+------------+----+\n",
            "|  session1|    /cart|          30|   1|\n",
            "|  session1|/products|          25|   2|\n",
            "|  session1|    /home|          10|   3|\n",
            "|  session2|   /about|          45|   1|\n",
            "|  session2|    /home|           5|   2|\n",
            "|  session3|    /home|           8|   1|\n",
            "+----------+---------+------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "web_logs_data = [\n",
        "    (\"session1\", \"/home\", 10), (\"session1\", \"/products\", 25),\n",
        "    (\"session1\", \"/cart\", 30), (\"session2\", \"/home\", 5),\n",
        "    (\"session2\", \"/about\", 45), (\"session3\", \"/home\", 8)\n",
        "]\n",
        "web_logs = spark.createDataFrame(web_logs_data, [\"session_id\", \"page\", \"duration_sec\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"session_id\").orderBy(desc(\"duration_sec\"))\n",
        "top_pages = web_logs.withColumn(\"rank\", rank().over(window_spec)).filter(col(\"rank\") <= 3)\n",
        "top_pages.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca8f357",
      "metadata": {
        "id": "6ca8f357"
      },
      "source": [
        "## 18. Streaming Platform: Total Watch Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "8eedb3bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eedb3bd",
        "outputId": "817a0811-49f6-48c3-830c-67a814af2b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|user_id|total_watch_min|\n",
            "+-------+---------------+\n",
            "|  user1|             75|\n",
            "|  user2|            120|\n",
            "|  user3|            195|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "watch_data = [\n",
        "    (\"user1\", 45), (\"user1\", 30), (\"user2\", 120),\n",
        "    (\"user3\", 90), (\"user3\", 60), (\"user3\", 45)\n",
        "]\n",
        "watch_logs = spark.createDataFrame(watch_data, [\"user_id\", \"duration_min\"])\n",
        "\n",
        "# Solution\n",
        "watch_time = watch_logs.groupBy(\"user_id\").agg(sum(\"duration_min\").alias(\"total_watch_min\"))\n",
        "watch_time.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af0e7a0",
      "metadata": {
        "id": "2af0e7a0"
      },
      "source": [
        "## 19. B2B SaaS: Churned Customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6097a897",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6097a897",
        "outputId": "ec5cf74c-d9d1-44aa-c874-ffad3c2c9e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|customer_id|last_login|\n",
            "+-----------+----------+\n",
            "|       C001|2023-09-01|\n",
            "|       C002|2023-08-15|\n",
            "|       C003|2023-10-05|\n",
            "|       C004|2023-07-20|\n",
            "+-----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "customers_data = [\n",
        "    (\"C001\", date(2023, 9, 1)),\n",
        "    (\"C002\", date(2023, 8, 15)),\n",
        "    (\"C003\", date(2023, 10, 5)),\n",
        "    (\"C004\", date(2023, 7, 20))\n",
        "]\n",
        "customers = spark.createDataFrame(customers_data, [\"customer_id\", \"last_login\"])\n",
        "\n",
        "# Solution\n",
        "churned = customers.filter(datediff(current_date(), col(\"last_login\")) > 90)\n",
        "churned.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df86d11",
      "metadata": {
        "id": "5df86d11"
      },
      "source": [
        "## 20. Job Portal: Location-Based Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8b4a258e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b4a258e",
        "outputId": "2f8d7408-79e7-4ed5-b9be-f1a1d1ae5cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------+\n",
            "|location|seeker_id|job_id|\n",
            "+--------+---------+------+\n",
            "|      CA|      JS2|   JP2|\n",
            "|      NY|      JS1|   JP1|\n",
            "+--------+---------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "job_seekers_data = [(\"JS1\", \"NY\"), (\"JS2\", \"CA\"), (\"JS3\", \"TX\")]\n",
        "job_postings_data = [(\"JP1\", \"NY\"), (\"JP2\", \"CA\"), (\"JP3\", \"FL\")]\n",
        "\n",
        "job_seekers = spark.createDataFrame(job_seekers_data, [\"seeker_id\", \"location\"])\n",
        "job_postings = spark.createDataFrame(job_postings_data, [\"job_id\", \"location\"])\n",
        "\n",
        "# Solution\n",
        "matches = job_seekers.join(job_postings, \"location\")\n",
        "matches.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8f05d2",
      "metadata": {
        "id": "8d8f05d2"
      },
      "source": [
        "## 21. Manufacturing: Machine Failure Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c190b952",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c190b952",
        "outputId": "ec3eabe2-856e-49b1-c9a2-e2ae02d537e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|machine_id|failure_count|\n",
            "+----------+-------------+\n",
            "+----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "machine_logs_data = [\n",
        "    (\"M001\", \"2025-06-22 08:00:00\", \"failure\"),\n",
        "    (\"M001\", \"2025-06-22 12:00:00\", \"failure\"),\n",
        "    (\"M001\", \"2025-06-21 09:00:00\", \"failure\"),\n",
        "    (\"M002\", \"2025-06-22 10:00:00\", \"failure\")\n",
        "]\n",
        "machine_logs = spark.createDataFrame(machine_logs_data, [\"machine_id\", \"timestamp\", \"status\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"machine_id\").orderBy(\"timestamp\")\n",
        "failure_counts = machine_logs.withColumn(\"prev_failure\", lag(\"timestamp\").over(window_spec)) \\\n",
        "    .withColumn(\"time_diff\",\n",
        "        (col(\"timestamp\").cast(\"long\") - col(\"prev_failure\").cast(\"long\")) / 3600) \\\n",
        "    .filter(col(\"time_diff\") <= 24) \\\n",
        "    .groupBy(\"machine_id\") \\\n",
        "    .agg(count(\"*\").alias(\"failure_count\")) \\\n",
        "    .filter(col(\"failure_count\") >= 2)\n",
        "failure_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b45882d",
      "metadata": {
        "id": "1b45882d"
      },
      "source": [
        "## 22. Insurance: Claim-to-Premium Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f86368e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86368e7",
        "outputId": "4914f7da-859e-49be-87d6-bdb60207707a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|policy_type|       claim_ratio|\n",
            "+-----------+------------------+\n",
            "|     Health|              0.25|\n",
            "|       Life|               0.3|\n",
            "|       Auto|0.6666666666666666|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "insurance_data = [\n",
        "    (\"Health\", 1000, 5000), (\"Health\", 1500, 5000),\n",
        "    (\"Auto\", 2000, 3000), (\"Life\", 3000, 10000)\n",
        "]\n",
        "insurance = spark.createDataFrame(insurance_data, [\"policy_type\", \"claim_amount\", \"premium\"])\n",
        "\n",
        "# Solution\n",
        "claim_ratio = insurance.groupBy(\"policy_type\") \\\n",
        "    .agg(\n",
        "        (sum(\"claim_amount\") / sum(\"premium\")).alias(\"claim_ratio\")\n",
        "    )\n",
        "claim_ratio.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6309fb",
      "metadata": {
        "id": "6b6309fb"
      },
      "source": [
        "## 23. Content Platform: Binge Watchers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "89f9225d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f9225d",
        "outputId": "5e14dd6d-421e-4f34-bf4a-13d96ab16ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------------+\n",
            "|user_id|show_id|consecutive_count|\n",
            "+-------+-------+-----------------+\n",
            "+-------+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "watch_data = [\n",
        "    (\"user1\", \"showA\", 1, \"2023-10-01 20:00:00\"),\n",
        "    (\"user1\", \"showA\", 2, \"2023-10-01 20:30:00\"),\n",
        "    (\"user1\", \"showA\", 3, \"2023-10-01 21:00:00\"),\n",
        "    (\"user2\", \"showB\", 1, \"2023-10-01 19:00:00\"),\n",
        "    (\"user2\", \"showB\", 2, \"2023-10-02 19:30:00\")\n",
        "]\n",
        "watch_logs = spark.createDataFrame(watch_data, [\"user_id\", \"show_id\", \"episode\", \"timestamp\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"user_id\", \"show_id\").orderBy(\"timestamp\")\n",
        "binge_watchers = watch_logs.withColumn(\"prev_ep\", lag(\"episode\").over(window_spec)) \\\n",
        "    .withColumn(\"time_diff\",\n",
        "        (col(\"timestamp\").cast(\"long\") - lag(col(\"timestamp\")).over(window_spec).cast(\"long\")) / 3600) \\\n",
        "    .filter((col(\"episode\") == col(\"prev_ep\") + 1) & (col(\"time_diff\") <= 2)) \\\n",
        "    .groupBy(\"user_id\", \"show_id\") \\\n",
        "    .agg(count(\"*\").alias(\"consecutive_count\")) \\\n",
        "    .filter(col(\"consecutive_count\") >= 2)\n",
        "binge_watchers.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e6638b",
      "metadata": {
        "id": "a4e6638b"
      },
      "source": [
        "## 24. Healthcare: Inconsistent Records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6693501c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6693501c",
        "outputId": "1b0e6722-51b1-46b5-d493-92e160128e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-------------------+\n",
            "|patient_id|         admit_time|     discharge_time|\n",
            "+----------+-------------------+-------------------+\n",
            "|      P002|2023-10-02 09:00:00|2023-10-01 14:00:00|\n",
            "+----------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "visits_data = [\n",
        "    (\"P001\", \"2023-10-01 10:00:00\", \"2023-10-01 15:00:00\"),\n",
        "    (\"P002\", \"2023-10-02 09:00:00\", \"2023-10-01 14:00:00\"),  # Inconsistent\n",
        "    (\"P003\", \"2023-10-03 11:00:00\", \"2023-10-03 16:00:00\")\n",
        "]\n",
        "visits = spark.createDataFrame(visits_data, [\"patient_id\", \"admit_time\", \"discharge_time\"]) \\\n",
        "    .withColumn(\"admit_time\", to_timestamp(col(\"admit_time\"))) \\\n",
        "    .withColumn(\"discharge_time\", to_timestamp(col(\"discharge_time\")))\n",
        "\n",
        "# Solution\n",
        "inconsistent = visits.filter(col(\"discharge_time\") < col(\"admit_time\"))\n",
        "inconsistent.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f6e78e",
      "metadata": {
        "id": "86f6e78e"
      },
      "source": [
        "## 25. Retail: Weekly Repeat Customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6f3fe941",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3fe941",
        "outputId": "0046dad8-42f1-470c-fbd9-789f5812565f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+------+\n",
            "|customer_id|week|visits|\n",
            "+-----------+----+------+\n",
            "+-----------+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "purchases_data = [\n",
        "    (\"C001\", \"2023-10-01\"), (\"C001\", \"2023-10-08\"),\n",
        "    (\"C002\", \"2023-10-02\"), (\"C003\", \"2023-10-01\"),\n",
        "    (\"C003\", \"2023-10-08\"), (\"C003\", \"2023-10-15\")\n",
        "]\n",
        "purchases = spark.createDataFrame(purchases_data, [\"customer_id\", \"purchase_date\"]) \\\n",
        "    .withColumn(\"purchase_date\", to_date(col(\"purchase_date\")))\n",
        "\n",
        "# Solution\n",
        "repeat_customers = purchases.groupBy(\"customer_id\", weekofyear(\"purchase_date\").alias(\"week\")) \\\n",
        "    .agg(count(\"*\").alias(\"visits\")) \\\n",
        "    .filter(col(\"visits\") > 1)\n",
        "repeat_customers.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150d4e15",
      "metadata": {
        "id": "150d4e15"
      },
      "source": [
        "## 26. HR Analytics: Employee Tenure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "80c26b37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80c26b37",
        "outputId": "1a21773b-6638-453f-c2a5-937115986cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-------------+\n",
            "|employee_id| join_date|tenure_months|\n",
            "+-----------+----------+-------------+\n",
            "|       E001|2020-05-15|  61.22580645|\n",
            "|       E002|2022-01-10|  41.38709677|\n",
            "|       E003|2018-11-20|  79.06451613|\n",
            "+-----------+----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "employees_data = [\n",
        "    (\"E001\", date(2020, 5, 15)),\n",
        "    (\"E002\", date(2022, 1, 10)),\n",
        "    (\"E003\", date(2018, 11, 20))\n",
        "]\n",
        "employees = spark.createDataFrame(employees_data, [\"employee_id\", \"join_date\"])\n",
        "\n",
        "# Solution\n",
        "tenure = employees.withColumn(\"tenure_months\",\n",
        "    months_between(current_date(), col(\"join_date\")))\n",
        "tenure.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ef61e8",
      "metadata": {
        "id": "b5ef61e8"
      },
      "source": [
        "## 27. Travel Agency: Destination Revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "450ec81c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450ec81c",
        "outputId": "33a503ee-8b47-4f02-80d0-e8523f81bbb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|destination|total_revenue|\n",
            "+-----------+-------------+\n",
            "|      Paris|         2400|\n",
            "|      Tokyo|         4500|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "bookings_data = [(\"B001\", \"Paris\", 2), (\"B002\", \"Tokyo\", 3)]\n",
        "packages_data = [(\"Paris\", 1200), (\"Tokyo\", 1500)]\n",
        "\n",
        "bookings = spark.createDataFrame(bookings_data, [\"booking_id\", \"destination\", \"num_travelers\"])\n",
        "packages = spark.createDataFrame(packages_data, [\"destination\", \"price_per_person\"])\n",
        "\n",
        "# Solution\n",
        "revenue = bookings.join(packages, \"destination\") \\\n",
        "    .withColumn(\"revenue\", col(\"price_per_person\") * col(\"num_travelers\")) \\\n",
        "    .groupBy(\"destination\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "revenue.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0e35ae",
      "metadata": {
        "id": "4c0e35ae"
      },
      "source": [
        "## 28. Ride-Sharing: Driver Cancellation Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ff985f27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff985f27",
        "outputId": "36cd0e48-c297-4c73-f074-119a790c9138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+--------------+------------------+\n",
            "|driver_id|total_trips|canceled_trips|       cancel_rate|\n",
            "+---------+-----------+--------------+------------------+\n",
            "|     D001|          3|             1|0.3333333333333333|\n",
            "|     D003|          1|             1|               1.0|\n",
            "+---------+-----------+--------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "trips_data = [\n",
        "    (\"D001\", \"completed\"), (\"D001\", \"canceled\"), (\"D001\", \"completed\"),\n",
        "    (\"D002\", \"completed\"), (\"D002\", \"completed\"), (\"D003\", \"canceled\")\n",
        "]\n",
        "trips = spark.createDataFrame(trips_data, [\"driver_id\", \"status\"])\n",
        "\n",
        "# Solution\n",
        "cancellation_rates = trips.groupBy(\"driver_id\").agg(\n",
        "    count(\"*\").alias(\"total_trips\"),\n",
        "    sum(when(col(\"status\") == \"canceled\", 1).otherwise(0)).alias(\"canceled_trips\"),\n",
        "    (sum(when(col(\"status\") == \"canceled\", 1).otherwise(0)) / count(\"*\")).alias(\"cancel_rate\")\n",
        ").filter(col(\"cancel_rate\") > 0.05)\n",
        "cancellation_rates.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff1749e",
      "metadata": {
        "id": "5ff1749e"
      },
      "source": [
        "## 29. Warehouse: Inventory Change Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "eb2dfc51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb2dfc51",
        "outputId": "4a7c0e96-413d-462d-f83c-6ac5b60b9df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----+----------+------------+\n",
            "|      date|product_id|stock|prev_stock|stock_change|\n",
            "+----------+----------+-----+----------+------------+\n",
            "|2023-10-01|      P100|   50|      NULL|        NULL|\n",
            "|2023-10-02|      P100|   45|        50|          -5|\n",
            "|2023-10-03|      P100|   40|        45|          -5|\n",
            "|2023-10-01|      P200|  100|      NULL|        NULL|\n",
            "|2023-10-02|      P200|  120|       100|          20|\n",
            "|2023-10-03|      P200|  110|       120|         -10|\n",
            "+----------+----------+-----+----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "inventory_data = [\n",
        "    (\"2023-10-01\", \"P100\", 50),\n",
        "    (\"2023-10-02\", \"P100\", 45),\n",
        "    (\"2023-10-03\", \"P100\", 40),\n",
        "    (\"2023-10-01\", \"P200\", 100),\n",
        "    (\"2023-10-02\", \"P200\", 120),\n",
        "    (\"2023-10-03\", \"P200\", 110)\n",
        "]\n",
        "inventory = spark.createDataFrame(inventory_data, [\"date\", \"product_id\", \"stock\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"product_id\").orderBy(\"date\")\n",
        "inventory_change = inventory.withColumn(\"prev_stock\", lag(\"stock\").over(window_spec)) \\\n",
        "    .withColumn(\"stock_change\", col(\"stock\") - col(\"prev_stock\"))\n",
        "inventory_change.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47088bab",
      "metadata": {
        "id": "47088bab"
      },
      "source": [
        "## 30. Education: Performance Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "65a21269",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65a21269",
        "outputId": "2764c502-67cd-4c3e-e3eb-6ff2d9178d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+----+-----+----------+-----------+\n",
            "|student_id|subject|exam|score|prev_score|improvement|\n",
            "+----------+-------+----+-----+----------+-----------+\n",
            "|      S001|   Math|   2|   85|        75|         10|\n",
            "|      S003|   Math|   2|   75|        60|         15|\n",
            "+----------+-------+----+-----+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "scores_data = [\n",
        "    (\"S001\", \"Math\", 1, 75), (\"S001\", \"Math\", 2, 85),\n",
        "    (\"S002\", \"Science\", 1, 80), (\"S002\", \"Science\", 2, 70),\n",
        "    (\"S003\", \"Math\", 1, 60), (\"S003\", \"Math\", 2, 75)\n",
        "]\n",
        "scores = spark.createDataFrame(scores_data, [\"student_id\", \"subject\", \"exam\", \"score\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"student_id\", \"subject\").orderBy(\"exam\")\n",
        "improving_students = scores.withColumn(\"prev_score\", lag(\"score\").over(window_spec)) \\\n",
        "    .filter(col(\"prev_score\").isNotNull()) \\\n",
        "    .withColumn(\"improvement\", col(\"score\") - col(\"prev_score\")) \\\n",
        "    .filter(col(\"improvement\") > 0)\n",
        "improving_students.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3964e93",
      "metadata": {
        "id": "b3964e93"
      },
      "source": [
        "## 31. Retail Banking: Monthly Spend Increase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "49d654d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49d654d1",
        "outputId": "b6f7ebff-6f78-444d-a547-a905e4b1d676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+-----------+------------+\n",
            "|user_id|  month|amount|prev_amount|pct_increase|\n",
            "+-------+-------+------+-----------+------------+\n",
            "|   U100|2023-02|  1800|       1000|         0.8|\n",
            "+-------+-------+------+-----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "spend_data = [\n",
        "    (\"U100\", \"2023-01\", 1000), (\"U100\", \"2023-02\", 1800),\n",
        "    (\"U200\", \"2023-01\", 500), (\"U200\", \"2023-02\", 700),\n",
        "    (\"U300\", \"2023-01\", 2000), (\"U300\", \"2023-02\", 2100)\n",
        "]\n",
        "spend = spark.createDataFrame(spend_data, [\"user_id\", \"month\", \"amount\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"month\")\n",
        "spend_increase = spend.withColumn(\"prev_amount\", lag(\"amount\").over(window_spec)) \\\n",
        "    .filter(col(\"prev_amount\").isNotNull()) \\\n",
        "    .withColumn(\"pct_increase\", (col(\"amount\") - col(\"prev_amount\")) / col(\"prev_amount\")) \\\n",
        "    .filter(col(\"pct_increase\") >= 0.5)\n",
        "spend_increase.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da543e7",
      "metadata": {
        "id": "9da543e7"
      },
      "source": [
        "## 32. IoT Sensors: Out-of-Range Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "38401648",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38401648",
        "outputId": "93be8347-b71c-45f6-8a87-d0b2d26fe0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|sensor_id|temperature|\n",
            "+---------+-----------+\n",
            "|     S002|       65.2|\n",
            "|     S004|       75.5|\n",
            "|     S005|       18.0|\n",
            "|     S006|       95.0|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "sensor_data = [\n",
        "    (\"S001\", 25.5), (\"S002\", 65.2), (\"S003\", 42.0),\n",
        "    (\"S004\", 75.5), (\"S005\", 18.0), (\"S006\", 95.0)\n",
        "]\n",
        "sensors = spark.createDataFrame(sensor_data, [\"sensor_id\", \"temperature\"])\n",
        "\n",
        "# Solution\n",
        "out_of_range = sensors.filter((col(\"temperature\") < 20) | (col(\"temperature\") > 60))\n",
        "out_of_range.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fb7183",
      "metadata": {
        "id": "a8fb7183"
      },
      "source": [
        "## 33. Sales Analysis: Duplicate Product Listings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "bd531901",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd531901",
        "outputId": "719d8eaa-fe66-4244-b77e-2faba8b88fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------------+-----+\n",
            "|   name|price| description|count|\n",
            "+-------+-----+------------+-----+\n",
            "|Phone X|  999|Latest model|    2|\n",
            "+-------+-----+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "products_data = [\n",
        "    (\"Phone X\", 999, \"Latest model\"),\n",
        "    (\"Phone X\", 999, \"Latest model\"),\n",
        "    (\"Laptop Pro\", 1500, \"High performance\"),\n",
        "    (\"Phone X\", 899, \"Refurbished model\")\n",
        "]\n",
        "products = spark.createDataFrame(products_data, [\"name\", \"price\", \"description\"])\n",
        "\n",
        "# Solution\n",
        "duplicates = products.groupBy(\"name\", \"price\", \"description\") \\\n",
        "    .count().filter(col(\"count\") > 1)\n",
        "duplicates.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72433fe5",
      "metadata": {
        "id": "72433fe5"
      },
      "source": [
        "## 34. Finance: PAN Masking UDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "267699d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267699d7",
        "outputId": "3042d97d-c4ac-46fa-ace5-786e8b3faad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+\n",
            "|user_id|       pan|masked_pan|\n",
            "+-------+----------+----------+\n",
            "|   U001|ABCDE1234F|ABXXXXXX4F|\n",
            "|   U002|FGHIJ5678K|FGXXXXXX8K|\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "users_data = [(\"U001\", \"ABCDE1234F\"), (\"U002\", \"FGHIJ5678K\")]\n",
        "users = spark.createDataFrame(users_data, [\"user_id\", \"pan\"])\n",
        "\n",
        "# Solution\n",
        "def mask_pan(pan):\n",
        "    return pan[:2] + \"XXXXXX\" + pan[-2:]\n",
        "\n",
        "mask_udf = udf(mask_pan, StringType())\n",
        "masked_users = users.withColumn(\"masked_pan\", mask_udf(col(\"pan\")))\n",
        "masked_users.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e15e61e",
      "metadata": {
        "id": "8e15e61e"
      },
      "source": [
        "## 35. Energy Sector: Daily Consumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4fb4cd82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fb4cd82",
        "outputId": "7855faea-9afe-456d-b6ac-29c5de5c35c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+--------+\n",
            "|      date|region|sum(kwh)|\n",
            "+----------+------+--------+\n",
            "|2023-10-01| South|    3200|\n",
            "|2023-10-01| North|    2500|\n",
            "|2023-10-02| North|    2600|\n",
            "|2023-10-02| South|    3100|\n",
            "+----------+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "energy_data = [\n",
        "    (\"2023-10-01\", \"North\", 2500),\n",
        "    (\"2023-10-01\", \"South\", 3200),\n",
        "    (\"2023-10-02\", \"North\", 2600),\n",
        "    (\"2023-10-02\", \"South\", 3100)\n",
        "]\n",
        "energy = spark.createDataFrame(energy_data, [\"date\", \"region\", \"kwh\"])\n",
        "\n",
        "# Solution\n",
        "daily_consumption = energy.groupBy(\"date\", \"region\").sum(\"kwh\")\n",
        "daily_consumption.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "947f03de",
      "metadata": {
        "id": "947f03de"
      },
      "source": [
        "## 36. Streaming: Running Count of Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3609b057",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3609b057",
        "outputId": "71c11ce0-09b3-436d-a994-34e14778a588"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dbutils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-44-3247314636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Simulated streaming source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstreaming_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tmp/streaming_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreaming_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# For Databricks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Generate sample file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
          ]
        }
      ],
      "source": [
        "# Simulated streaming source\n",
        "streaming_path = \"/tmp/streaming_data\"\n",
        "dbutils.fs.rm(streaming_path, True)  # For Databricks\n",
        "\n",
        "# Generate sample file\n",
        "sample_data = [(\"event1\",), (\"event2\",)]\n",
        "spark.createDataFrame(sample_data, [\"event_id\"]).write.csv(streaming_path, header=True)\n",
        "\n",
        "# Stream processing\n",
        "schema = StructType([StructField(\"event_id\", StringType())])\n",
        "stream_df = spark.readStream.schema(schema).csv(streaming_path)\n",
        "running_count = stream_df.groupBy().count()\n",
        "\n",
        "# Start stream\n",
        "query = running_count.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(10)\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47ac5e8",
      "metadata": {
        "id": "f47ac5e8"
      },
      "source": [
        "## 37. ETL: SCD Type 2 Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f4dfe5",
      "metadata": {
        "id": "05f4dfe5"
      },
      "outputs": [],
      "source": [
        "# Create Delta table\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS products_scd2 (product_id INT, name STRING, price DECIMAL(10,2), start_date DATE, end_date DATE, is_current BOOLEAN) USING DELTA\")\n",
        "\n",
        "# Initial data\n",
        "initial_data = [(1, \"Phone X\", 999.99, date(2023, 1, 1), date(9999, 12, 31), True)]\n",
        "spark.createDataFrame(initial_data).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products_scd2\")\n",
        "\n",
        "# New data\n",
        "new_data = [\n",
        "    (1, \"Phone X\", 899.99),  # Price update\n",
        "    (2, \"Tablet Pro\", 599.99)  # New product\n",
        "]\n",
        "new_df = spark.createDataFrame(new_data, [\"product_id\", \"name\", \"price\"])\n",
        "\n",
        "# SCD Type 2 Merge\n",
        "current_table = DeltaTable.forName(spark, \"products_scd2\")\n",
        "\n",
        "(current_table.alias(\"t\")\n",
        " .merge(new_df.alias(\"s\"), \"t.product_id = s.product_id\")\n",
        " .whenMatchedUpdate(\n",
        "     condition = \"t.price <> s.price AND t.is_current = true\",\n",
        "     set = {\n",
        "         \"end_date\": current_date(),\n",
        "         \"is_current\": False\n",
        "     })\n",
        " .whenNotMatchedInsert(\n",
        "     values = {\n",
        "         \"product_id\": \"s.product_id\",\n",
        "         \"name\": \"s.name\",\n",
        "         \"price\": \"s.price\",\n",
        "         \"start_date\": current_date(),\n",
        "         \"end_date\": date(9999, 12, 31),\n",
        "         \"is_current\": True\n",
        "     })\n",
        " .execute())\n",
        "\n",
        "spark.sql(\"SELECT * FROM products_scd2\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3523a23b",
      "metadata": {
        "id": "3523a23b"
      },
      "source": [
        "## 38. Data Lake: CSV to Partitioned Parquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dbutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKpuVvpkHp6d",
        "outputId": "b7ebb9d8-c34d-4f85-f5e2-f133b4907c57"
      },
      "id": "KKpuVvpkHp6d",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dbutils\n",
            "  Downloading dbutils-3.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading dbutils-3.1.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: dbutils\n",
            "Successfully installed dbutils-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "f0130453",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "f0130453",
        "outputId": "3ff3c8bd-aa0e-4657-af97-25f93220eac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitioned files:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dbutils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-48-624174050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Verify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Partitioned files:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/temp/result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# For Databricks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dbutils' is not defined"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "daily_data = [\n",
        "    (\"2023-10-01\", \"A100\", 25), (\"2023-10-01\", \"B200\", 40),\n",
        "    (\"2023-10-02\", \"A100\", 30), (\"2023-10-02\", \"C300\", 15)\n",
        "]\n",
        "daily_sales = spark.createDataFrame(daily_data, [\"date\", \"product_id\", \"sales\"])\n",
        "\n",
        "# Write as partitioned Parquet\n",
        "daily_sales.write.partitionBy(\"date\").parquet(\"/content/sample_data/result\")\n",
        "\n",
        "# Verify\n",
        "print(\"Partitioned files:\")\n",
        "display(dbutils.fs.ls(\"/content/sample_data/temp/result\"))  # For Databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794dbaa9",
      "metadata": {
        "id": "794dbaa9"
      },
      "source": [
        "## 39. Data Governance: Schema Drift Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0b4904fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4904fe",
        "outputId": "7fff7c12-ed0d-45d7-921d-d2367dcd6fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema Drift Report: {'added': ['full_name', 'email'], 'removed': ['name'], 'changed': []}\n"
          ]
        }
      ],
      "source": [
        "# Sample schemas\n",
        "schema_v1 = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"name\", StringType())\n",
        "])\n",
        "\n",
        "schema_v2 = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"full_name\", StringType()),  # Changed column\n",
        "    StructField(\"email\", StringType())  # New column\n",
        "])\n",
        "\n",
        "# Detection function\n",
        "def detect_schema_drift(current_schema, new_schema):\n",
        "    current_fields = {f.name: f.dataType for f in current_schema}\n",
        "    new_fields = {f.name: f.dataType for f in new_schema}\n",
        "\n",
        "    added = [col for col in new_fields if col not in current_fields]\n",
        "    removed = [col for col in current_fields if col not in new_fields]\n",
        "    changed = [\n",
        "        col for col in current_fields\n",
        "        if col in new_fields and current_fields[col] != new_fields[col]\n",
        "    ]\n",
        "\n",
        "    return {\"added\": added, \"removed\": removed, \"changed\": changed}\n",
        "\n",
        "# Test detection\n",
        "drift_report = detect_schema_drift(schema_v1, schema_v2)\n",
        "print(f\"Schema Drift Report: {drift_report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa14ef06",
      "metadata": {
        "id": "aa14ef06"
      },
      "source": [
        "## 40. Power BI: Sales Aggregation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "7110c099",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7110c099",
        "outputId": "bf0bc776-2db3-490e-ce03-5ffd12edba75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold Layer:\n",
            "+----------+----------+---------+-----------+\n",
            "|      date|product_id|total_qty|total_sales|\n",
            "+----------+----------+---------+-----------+\n",
            "|2023-10-01|      P100|        2|      51.98|\n",
            "|2023-10-02|      P100|        3|      77.97|\n",
            "|2023-10-01|      P200|        1|      49.99|\n",
            "+----------+----------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Raw data\n",
        "raw_sales_data = [\n",
        "    (1001, \"2023-10-01\", \"P100\", 2, 25.99),\n",
        "    (1002, \"2023-10-01\", \"P200\", 1, 49.99),\n",
        "    (1003, \"2023-10-02\", \"P100\", 3, 25.99)\n",
        "]\n",
        "raw_sales = spark.createDataFrame(raw_sales_data, [\"order_id\", \"date\", \"product_id\", \"qty\", \"price\"])\n",
        "\n",
        "# Silver layer (cleaned)\n",
        "silver_sales = raw_sales.dropna().filter(col(\"qty\") > 0)\n",
        "\n",
        "# Gold layer (aggregated)\n",
        "gold_sales = silver_sales.groupBy(\"date\", \"product_id\").agg(\n",
        "    sum(\"qty\").alias(\"total_qty\"),\n",
        "    sum(col(\"qty\") * col(\"price\")).alias(\"total_sales\")\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"Gold Layer:\")\n",
        "gold_sales.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc8804c",
      "metadata": {
        "id": "ecc8804c"
      },
      "source": [
        "## 41. Security Monitoring: Failed Login Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "e7d9b957",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7d9b957",
        "outputId": "daed27f9-b4fb-4734-87d1-a00ab6abe04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------------+------+----------+\n",
            "|username|          timestamp|status|fail_count|\n",
            "+--------+-------------------+------+----------+\n",
            "|   user1|2023-10-01 09:00:15|  fail|         3|\n",
            "+--------+-------------------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "login_data = [\n",
        "    (\"user1\", \"2023-10-01 09:00:00\", \"success\"),\n",
        "    (\"user1\", \"2023-10-01 09:00:05\", \"fail\"),\n",
        "    (\"user1\", \"2023-10-01 09:00:10\", \"fail\"),\n",
        "    (\"user1\", \"2023-10-01 09:00:15\", \"fail\"),\n",
        "    (\"user2\", \"2023-10-01 10:00:00\", \"success\")\n",
        "]\n",
        "logins = spark.createDataFrame(login_data, [\"username\", \"timestamp\", \"status\"]) \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"username\").orderBy(\"timestamp\").rowsBetween(-2, 0)\n",
        "failed_logins = logins.filter(col(\"status\") == \"fail\") \\\n",
        "    .withColumn(\"fail_count\", count(\"*\").over(window_spec)) \\\n",
        "    .filter(col(\"fail_count\") >= 3)\n",
        "failed_logins.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ab7801",
      "metadata": {
        "id": "c6ab7801"
      },
      "source": [
        "## 42. IoT Analytics: 15-min Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "aa01a803",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa01a803",
        "outputId": "fe87acef-7e59-4cff-d867-00eb0b9635dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-------------------+---------+\n",
            "|sensor_id|              start|                end|avg_value|\n",
            "+---------+-------------------+-------------------+---------+\n",
            "|     S001|2023-10-01 08:00:00|2023-10-01 08:15:00|    25.75|\n",
            "|     S001|2023-10-01 08:15:00|2023-10-01 08:30:00|     26.0|\n",
            "+---------+-------------------+-------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "sensor_data = [\n",
        "    (\"S001\", \"2023-10-01 08:05:00\", 25.5),\n",
        "    (\"S001\", \"2023-10-01 08:10:00\", 26.0),\n",
        "    (\"S001\", \"2023-10-01 08:15:00\", 25.8),\n",
        "    (\"S001\", \"2023-10-01 08:20:00\", 26.2)\n",
        "]\n",
        "sensor_logs = spark.createDataFrame(sensor_data, [\"sensor_id\", \"timestamp\", \"value\"]) \\\n",
        "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
        "\n",
        "# Solution\n",
        "agg_15min = sensor_logs.groupBy(\n",
        "    \"sensor_id\",\n",
        "    window(\"timestamp\", \"15 minutes\")\n",
        ").agg(avg(\"value\").alias(\"avg_value\"))\n",
        "\n",
        "agg_15min.select(\"sensor_id\", \"window.start\", \"window.end\", \"avg_value\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabb1846",
      "metadata": {
        "id": "dabb1846"
      },
      "source": [
        "## 43. Call Center: Peak Call Hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "9c83dbcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c83dbcf",
        "outputId": "6acce04d-8f20-432d-a8bd-b104d5f8bc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----+-----+\n",
            "|day_of_week|hour|count|\n",
            "+-----------+----+-----+\n",
            "|          2|  14|    3|\n",
            "|          1|   9|    2|\n",
            "|          1|  10|    1|\n",
            "+-----------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Corrected Sample Data\n",
        "calls_data = [\n",
        "    (\"2023-10-01 09:15:00\",), (\"2023-10-01 09:30:00\",),\n",
        "    (\"2023-10-01 10:00:00\",), (\"2023-10-02 14:15:00\",),\n",
        "    (\"2023-10-02 14:30:00\",), (\"2023-10-02 14:45:00\",)\n",
        "]\n",
        "\n",
        "calls = spark.createDataFrame(calls_data, [\"call_time\"]) \\\n",
        "    .withColumn(\"call_time\", to_timestamp(col(\"call_time\")))\n",
        "\n",
        "# Solution\n",
        "peak_hours = calls.groupBy(\n",
        "    dayofweek(\"call_time\").alias(\"day_of_week\"),\n",
        "    hour(\"call_time\").alias(\"hour\")\n",
        ").count().orderBy(desc(\"count\"))\n",
        "\n",
        "peak_hours.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ec291a",
      "metadata": {
        "id": "b3ec291a"
      },
      "source": [
        "## 44. CRM: Customer Data Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "7df31423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7df31423",
        "outputId": "5ba130f2-135f-40c8-da3d-73f132361804"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o24.sql.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: DELTA. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:605)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:165)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:52)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:52)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: DELTA.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 64 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-55-875989743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create Delta table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE IF NOT EXISTS customers_delta (customer_id INT, name STRING, city STRING, last_updated TIMESTAMP) USING DELTA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# New data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m new_data = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o24.sql.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: DELTA. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:605)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:165)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:52)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:110)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:52)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:46)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: DELTA.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 64 more\n"
          ]
        }
      ],
      "source": [
        "# Create Delta table\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS customers_delta (customer_id INT, name STRING, city STRING, last_updated TIMESTAMP) USING DELTA\")\n",
        "\n",
        "# New data\n",
        "new_data = [\n",
        "    (1, \"John Smith\", \"New York\", current_timestamp()),  # Update\n",
        "    (2, \"Sarah Lee\", \"London\", current_timestamp())     # Insert\n",
        "]\n",
        "new_df = spark.createDataFrame(new_data, [\"customer_id\", \"name\", \"city\", \"last_updated\"])\n",
        "\n",
        "# Merge operation\n",
        "delta_table = DeltaTable.forName(spark, \"customers_delta\")\n",
        "\n",
        "(delta_table.alias(\"t\")\n",
        " .merge(new_df.alias(\"s\"), \"t.customer_id = s.customer_id\")\n",
        " .whenMatchedUpdateAll()\n",
        " .whenNotMatchedInsertAll()\n",
        " .execute())\n",
        "\n",
        "spark.sql(\"SELECT * FROM customers_delta\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065b2c13",
      "metadata": {
        "id": "065b2c13"
      },
      "source": [
        "## 45. Marketing Funnel: User Journey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "8c1522d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1522d1",
        "outputId": "12018e7c-f82e-4050-db75-cf410abc7ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|   event|users|\n",
            "+--------+-----+\n",
            "|    view|    3|\n",
            "|   click|    2|\n",
            "|purchase|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "events_data = [\n",
        "    (\"user1\", \"view\", \"2023-10-01 09:00:00\"),\n",
        "    (\"user1\", \"click\", \"2023-10-01 09:01:00\"),\n",
        "    (\"user1\", \"purchase\", \"2023-10-01 09:05:00\"),\n",
        "    (\"user2\", \"view\", \"2023-10-01 10:00:00\"),\n",
        "    (\"user3\", \"view\", \"2023-10-01 11:00:00\"),\n",
        "    (\"user3\", \"click\", \"2023-10-01 11:02:00\")\n",
        "]\n",
        "events = spark.createDataFrame(events_data, [\"user_id\", \"event\", \"timestamp\"])\n",
        "\n",
        "# Solution - Funnel conversion\n",
        "funnel_stages = [\"view\", \"click\", \"purchase\"]\n",
        "funnel_counts = events.groupBy(\"event\").agg(countDistinct(\"user_id\").alias(\"users\")) \\\n",
        "    .filter(col(\"event\").isin(funnel_stages)) \\\n",
        "    .orderBy(expr(f\"array_position(array('view','click','purchase'), event)\"))\n",
        "\n",
        "funnel_counts.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99750476",
      "metadata": {
        "id": "99750476"
      },
      "source": [
        "## 46. Sports Analytics: Performance Trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "bd30f9b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd30f9b6",
        "outputId": "99cae624-0144-4d22-eeba-b479c5f384da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+---------------+---------------+\n",
            "| player|avg_improvement|min_improvement|max_improvement|\n",
            "+-------+---------------+---------------+---------------+\n",
            "|PlayerA|            3.5|              3|              4|\n",
            "|PlayerB|            1.0|             -3|              5|\n",
            "+-------+---------------+---------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "player_data = [\n",
        "    (\"PlayerA\", \"Season1\", 85), (\"PlayerA\", \"Season2\", 88), (\"PlayerA\", \"Season3\", 92),\n",
        "    (\"PlayerB\", \"Season1\", 78), (\"PlayerB\", \"Season2\", 75), (\"PlayerB\", \"Season3\", 80)\n",
        "]\n",
        "players = spark.createDataFrame(player_data, [\"player\", \"season\", \"avg_score\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"player\").orderBy(\"season\")\n",
        "trend_analysis = players.withColumn(\"prev_score\", lag(\"avg_score\").over(window_spec)) \\\n",
        "    .filter(col(\"prev_score\").isNotNull()) \\\n",
        "    .withColumn(\"improvement\", col(\"avg_score\") - col(\"prev_score\")) \\\n",
        "    .groupBy(\"player\") \\\n",
        "    .agg(\n",
        "        avg(\"improvement\").alias(\"avg_improvement\"),\n",
        "        min(\"improvement\").alias(\"min_improvement\"),\n",
        "        max(\"improvement\").alias(\"max_improvement\")\n",
        "    )\n",
        "trend_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "341dece2",
      "metadata": {
        "id": "341dece2"
      },
      "source": [
        "## 47. Transportation: Bus Utilization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "81dadd88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81dadd88",
        "outputId": "3b58bcf5-8d6f-4d5e-b565-ac1e77c97bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|bus_id|    avg_utilization|\n",
            "+------+-------------------+\n",
            "|Bus003|0.16666666666666666|\n",
            "+------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "bus_data = [\n",
        "    (\"Bus001\", 50, 15), (\"Bus001\", 50, 20),\n",
        "    (\"Bus002\", 40, 35), (\"Bus003\", 60, 10)\n",
        "]\n",
        "buses = spark.createDataFrame(bus_data, [\"bus_id\", \"capacity\", \"passengers\"])\n",
        "\n",
        "# Solution\n",
        "utilization = buses.withColumn(\"utilization\", col(\"passengers\") / col(\"capacity\")) \\\n",
        "    .groupBy(\"bus_id\") \\\n",
        "    .agg(avg(\"utilization\").alias(\"avg_utilization\")) \\\n",
        "    .filter(col(\"avg_utilization\") < 0.3)\n",
        "utilization.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcc68e35",
      "metadata": {
        "id": "bcc68e35"
      },
      "source": [
        "## 48. Retail Chains: Price Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "2178d74c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2178d74c",
        "outputId": "12171394-8c47-4990-8970-a6bafe019339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+-----------+\n",
            "| product|      price_stddev|price_range|\n",
            "+--------+------------------+-----------+\n",
            "|ProductA|0.7637626158259734|        1.5|\n",
            "|ProductB|0.2886751345948129|        0.5|\n",
            "+--------+------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "pricing_data = [\n",
        "    (\"ProductA\", \"Store1\", 10.99), (\"ProductA\", \"Store2\", 9.99),\n",
        "    (\"ProductA\", \"Store3\", 11.49), (\"ProductB\", \"Store1\", 24.99),\n",
        "    (\"ProductB\", \"Store2\", 25.49), (\"ProductB\", \"Store3\", 24.99)\n",
        "]\n",
        "pricing = spark.createDataFrame(pricing_data, [\"product\", \"store\", \"price\"])\n",
        "\n",
        "# Solution\n",
        "price_variance = pricing.groupBy(\"product\").agg(\n",
        "    stddev(\"price\").alias(\"price_stddev\"),\n",
        "    (max(\"price\") - min(\"price\")).alias(\"price_range\")\n",
        ")\n",
        "price_variance.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6137e638",
      "metadata": {
        "id": "6137e638"
      },
      "source": [
        "## 49. Cloud Billing: Cost Spikes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "025ce9b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "025ce9b3",
        "outputId": "fbf46552-d5f9-4e4b-95a6-a923c7e4c21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+----------+\n",
            "|   week|service|cost|prev_cost|pct_change|\n",
            "+-------+-------+----+---------+----------+\n",
            "|2023-41|Compute|3000|     1000|       2.0|\n",
            "+-------+-------+----+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "billing_data = [\n",
        "    (\"2023-40\", \"Compute\", 1000), (\"2023-41\", \"Compute\", 3000),\n",
        "    (\"2023-40\", \"Storage\", 500), (\"2023-41\", \"Storage\", 550)\n",
        "]\n",
        "billing = spark.createDataFrame(billing_data, [\"week\", \"service\", \"cost\"])\n",
        "\n",
        "# Solution\n",
        "window_spec = Window.partitionBy(\"service\").orderBy(\"week\")\n",
        "cost_spikes = billing.withColumn(\"prev_cost\", lag(\"cost\").over(window_spec)) \\\n",
        "    .filter(col(\"prev_cost\").isNotNull()) \\\n",
        "    .withColumn(\"pct_change\", (col(\"cost\") - col(\"prev_cost\")) / col(\"prev_cost\")) \\\n",
        "    .filter(col(\"pct_change\") > 1.0)  # >100% increase\n",
        "cost_spikes.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f77c8c",
      "metadata": {
        "id": "81f77c8c"
      },
      "source": [
        "## 50. Customer Experience: NPS Trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "a5e79323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5e79323",
        "outputId": "a6bc4f79-5906-4ef0-94b7-ba257d05432e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+-------+\n",
            "|region|              start|avg_nps|\n",
            "+------+-------------------+-------+\n",
            "| North|2023-09-28 00:00:00|    8.0|\n",
            "| North|2023-10-05 00:00:00|    7.0|\n",
            "| North|2023-10-12 00:00:00|    9.0|\n",
            "| South|2023-09-28 00:00:00|    9.0|\n",
            "| South|2023-10-05 00:00:00|    8.0|\n",
            "| South|2023-10-12 00:00:00|    9.0|\n",
            "+------+-------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Data\n",
        "nps_data = [\n",
        "    (\"2023-10-01\", \"North\", 8), (\"2023-10-01\", \"South\", 9),\n",
        "    (\"2023-10-08\", \"North\", 7), (\"2023-10-08\", \"South\", 8),\n",
        "    (\"2023-10-15\", \"North\", 9), (\"2023-10-15\", \"South\", 9)\n",
        "]\n",
        "nps = spark.createDataFrame(nps_data, [\"date\", \"region\", \"score\"])\n",
        "\n",
        "# Solution\n",
        "nps_trends = nps.groupBy(\"region\", window(\"date\", \"1 week\")).agg(\n",
        "    avg(\"score\").alias(\"avg_nps\")\n",
        ").orderBy(\"region\", \"window.start\")\n",
        "nps_trends.select(\"region\", \"window.start\", \"avg_nps\").show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}